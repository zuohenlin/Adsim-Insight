#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BroadTopicExtractionæ¨¡å— - ä¸»ç¨‹åº
æ•´åˆè¯é¢˜æå–çš„å®Œæ•´å·¥ä½œæµç¨‹å’Œå‘½ä»¤è¡Œå·¥å…·
"""

import sys
import asyncio
import argparse
from datetime import datetime, date
from pathlib import Path
from typing import List, Dict, Optional
from loguru import logger

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

try:
    from BroadTopicExtraction.get_today_news import NewsCollector, SOURCE_NAMES
    from BroadTopicExtraction.topic_extractor import TopicExtractor
    from BroadTopicExtraction.database_manager import DatabaseManager
except ImportError as e:
    logger.exception(f"å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    logger.error("è¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œï¼Œå¹¶ä¸”å·²å®‰è£…æ‰€æœ‰ä¾èµ–")
    sys.exit(1)

class BroadTopicExtraction:
    """BroadTopicExtractionä¸»è¦å·¥ä½œæµç¨‹"""
    
    def __init__(self):
        """åˆå§‹åŒ–"""
        self.news_collector = NewsCollector()
        self.topic_extractor = TopicExtractor()
        self.db_manager = DatabaseManager()
        
        logger.info("BroadTopicExtraction åˆå§‹åŒ–å®Œæˆ")
    
    def close(self):
        """å…³é—­èµ„æº"""
        if self.news_collector:
            self.news_collector.close()
        if self.db_manager:
            self.db_manager.close()
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()
    
    async def __aenter__(self):
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        self.close()
    
    async def run_daily_extraction(self, 
                                  news_sources: Optional[List[str]] = None,
                                  max_keywords: int = 100) -> Dict:
        """
        è¿è¡Œæ¯æ—¥è¯é¢˜æå–æµç¨‹
        
        Args:
            news_sources: æ–°é—»æºåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰æ”¯æŒçš„æº
            max_keywords: æœ€å¤§å…³é”®è¯æ•°é‡
            
        Returns:
            åŒ…å«å®Œæ•´æå–ç»“æœçš„å­—å…¸
        """
        extraction_result_message = ""
        extraction_result_message += "\nMindSpider AIçˆ¬è™« - æ¯æ—¥è¯é¢˜æå–\n"
        extraction_result_message += f"æ‰§è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        extraction_result_message += f"ç›®æ ‡æ—¥æœŸ: {date.today()}\n"
        
        if news_sources:
            extraction_result_message += f"æŒ‡å®šå¹³å°: {len(news_sources)} ä¸ª\n"
            for source in news_sources:
                source_name = SOURCE_NAMES.get(source, source)
                extraction_result_message += f"  - {source_name}\n"
        else:
            extraction_result_message += f"çˆ¬å–å¹³å°: å…¨éƒ¨ {len(SOURCE_NAMES)} ä¸ªå¹³å°\n"
        
        extraction_result_message += f"å…³é”®è¯æ•°: æœ€å¤š {max_keywords} ä¸ª\n"
        
        logger.info(extraction_result_message)
        
        extraction_result = {
            'success': False,
            'extraction_date': date.today().isoformat(),
            'start_time': datetime.now().isoformat(),
            'news_collection': {},
            'topic_extraction': {},
            'database_save': {},
            'error': None
        }
        
        try:
            # æ­¥éª¤1: æ”¶é›†æ–°é—»
            logger.info("ã€æ­¥éª¤1ã€‘æ”¶é›†çƒ­ç‚¹æ–°é—»...")
            news_result = await self.news_collector.collect_and_save_news(
                sources=news_sources
            )
            
            extraction_result['news_collection'] = {
                'success': news_result['success'],
                'total_news': news_result.get('total_news', 0),
                'successful_sources': news_result.get('successful_sources', 0),
                'total_sources': news_result.get('total_sources', 0)
            }
            
            if not news_result['success'] or not news_result['news_list']:
                raise Exception("æ–°é—»æ”¶é›†å¤±è´¥æˆ–æ²¡æœ‰è·å–åˆ°æ–°é—»")
            
            # æ­¥éª¤2: æå–å…³é”®è¯å’Œç”Ÿæˆæ€»ç»“
            logger.info("ã€æ­¥éª¤2ã€‘æå–å…³é”®è¯å’Œç”Ÿæˆæ€»ç»“...")
            keywords, summary = self.topic_extractor.extract_keywords_and_summary(
                news_result['news_list'], 
                max_keywords=max_keywords
            )
            
            extraction_result['topic_extraction'] = {
                'success': len(keywords) > 0,
                'keywords_count': len(keywords),
                'keywords': keywords,
                'summary': summary
            }
            
            if not keywords:
                logger.warning("è­¦å‘Š: æ²¡æœ‰æå–åˆ°æœ‰æ•ˆå…³é”®è¯")
            
            # æ­¥éª¤3: ä¿å­˜åˆ°æ•°æ®åº“
            logger.info("ã€æ­¥éª¤3ã€‘ä¿å­˜åˆ†æç»“æœåˆ°æ•°æ®åº“...")
            save_success = self.db_manager.save_daily_topics(
                keywords, summary, date.today()
            )
            
            extraction_result['database_save'] = {
                'success': save_success
            }
            
            extraction_result['success'] = True
            extraction_result['end_time'] = datetime.now().isoformat()
            
            logger.info("æ¯æ—¥è¯é¢˜æå–æµç¨‹å®Œæˆ!")
            
            return extraction_result
            
        except Exception as e:
            logger.exception(f"è¯é¢˜æå–æµç¨‹å¤±è´¥: {e}")
            extraction_result['error'] = str(e)
            extraction_result['end_time'] = datetime.now().isoformat()
            return extraction_result
    
    def print_extraction_results(self, extraction_result: Dict):
        """æ‰“å°æå–ç»“æœ"""
        extraction_result_message = ""
        
        # æ–°é—»æ”¶é›†ç»“æœ
        news_data = extraction_result.get('news_collection', {})
        extraction_result_message += f"\nğŸ“° æ–°é—»æ”¶é›†: {news_data.get('total_news', 0)} æ¡æ–°é—»\n"
        extraction_result_message += f"   æˆåŠŸæºæ•°: {news_data.get('successful_sources', 0)}/{news_data.get('total_sources', 0)}\n"
        
        # è¯é¢˜æå–ç»“æœ
        topic_data = extraction_result.get('topic_extraction', {})
        keywords = topic_data.get('keywords', [])
        summary = topic_data.get('summary', '')
        
        extraction_result_message += f"\nğŸ”‘ æå–å…³é”®è¯: {len(keywords)} ä¸ª\n"
        if keywords:
            # æ¯è¡Œæ˜¾ç¤º5ä¸ªå…³é”®è¯
            for i in range(0, len(keywords), 5):
                keyword_group = keywords[i:i+5]
                extraction_result_message += f"   {', '.join(keyword_group)}\n"
        
        extraction_result_message += f"\nğŸ“ æ–°é—»æ€»ç»“:\n   {summary}\n"
        
        # æ•°æ®åº“ä¿å­˜ç»“æœ
        db_data = extraction_result.get('database_save', {})
        if db_data.get('success'):
            extraction_result_message += f"\nğŸ’¾ æ•°æ®åº“ä¿å­˜: æˆåŠŸ\n"
        else:
            extraction_result_message += f"\nğŸ’¾ æ•°æ®åº“ä¿å­˜: å¤±è´¥\n"
        
        logger.info(extraction_result_message)
    
    def get_keywords_for_crawling(self, extract_date: date = None) -> List[str]:
        """
        è·å–ç”¨äºçˆ¬å–çš„å…³é”®è¯åˆ—è¡¨
        
        Args:
            extract_date: æå–æ—¥æœŸï¼Œé»˜è®¤ä¸ºä»Šå¤©
            
        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        try:
            # ä»æ•°æ®åº“è·å–è¯é¢˜åˆ†æ
            topics_data = self.db_manager.get_daily_topics(extract_date)
            
            if not topics_data:
                logger.info(f"æ²¡æœ‰æ‰¾åˆ° {extract_date or date.today()} çš„è¯é¢˜æ•°æ®")
                return []
            
            keywords = topics_data['keywords']
            
            # ç”Ÿæˆæœç´¢å…³é”®è¯
            search_keywords = self.topic_extractor.get_search_keywords(keywords)
            
            logger.info(f"å‡†å¤‡äº† {len(search_keywords)} ä¸ªå…³é”®è¯ç”¨äºçˆ¬å–")
            return search_keywords
            
        except Exception as e:
            logger.error(f"è·å–çˆ¬å–å…³é”®è¯å¤±è´¥: {e}")
            return []
    
    def get_daily_analysis(self, target_date: date = None) -> Optional[Dict]:
        """è·å–æŒ‡å®šæ—¥æœŸçš„åˆ†æç»“æœ"""
        try:
            return self.db_manager.get_daily_topics(target_date)
        except Exception as e:
            logger.error(f"è·å–æ¯æ—¥åˆ†æå¤±è´¥: {e}")
            return None
    
    def get_recent_analysis(self, days: int = 7) -> List[Dict]:
        """è·å–æœ€è¿‘å‡ å¤©çš„åˆ†æç»“æœ"""
        try:
            return self.db_manager.get_recent_topics(days)
        except Exception as e:
            logger.error(f"è·å–æœ€è¿‘åˆ†æå¤±è´¥: {e}")
            return []

# ==================== å‘½ä»¤è¡Œå·¥å…· ====================

async def run_extraction_command(sources=None, keywords_count=100, show_details=True):
    """è¿è¡Œè¯é¢˜æå–å‘½ä»¤"""
    
    try:
        async with BroadTopicExtraction() as extractor:
            # è¿è¡Œè¯é¢˜æå–
            result = await extractor.run_daily_extraction(
                news_sources=sources,
                max_keywords=keywords_count
            )
            
            if result['success']:
                if show_details:
                    # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
                    extractor.print_extraction_results(result)
                else:
                    # åªæ˜¾ç¤ºç®€è¦ç»“æœ
                    news_data = result.get('news_collection', {})
                    topic_data = result.get('topic_extraction', {})
                    
                    logger.info(f"âœ… è¯é¢˜æå–æˆåŠŸå®Œæˆ!")
                    logger.info(f"   æ”¶é›†æ–°é—»: {news_data.get('total_news', 0)} æ¡")
                    logger.info(f"   æå–å…³é”®è¯: {len(topic_data.get('keywords', []))} ä¸ª")
                    logger.info(f"   ç”Ÿæˆæ€»ç»“: {len(topic_data.get('summary', ''))} å­—ç¬¦")
                
                # è·å–çˆ¬å–å…³é”®è¯
                crawling_keywords = extractor.get_keywords_for_crawling()
                
                if crawling_keywords:
                    logger.info(f"\nğŸ”‘ ä¸ºDeepSentimentCrawlingå‡†å¤‡çš„æœç´¢å…³é”®è¯:")
                    logger.info(f"   {', '.join(crawling_keywords)}")
                    
                    # ä¿å­˜å…³é”®è¯åˆ°æ–‡ä»¶
                    keywords_file = project_root / "data" / "daily_keywords.txt"
                    keywords_file.parent.mkdir(exist_ok=True)
                    
                    with open(keywords_file, 'w', encoding='utf-8') as f:
                        f.write('\n'.join(crawling_keywords))
                    
                    logger.info(f"   å…³é”®è¯å·²ä¿å­˜åˆ°: {keywords_file}")
                
                return True
                
            else:
                logger.error(f"âŒ è¯é¢˜æå–å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                return False
                
    except Exception as e:
        logger.error(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="MindSpideræ¯æ—¥è¯é¢˜æå–å·¥å…·")
    parser.add_argument("--sources", nargs="+", help="æŒ‡å®šæ–°é—»æºå¹³å°", 
                       choices=list(SOURCE_NAMES.keys()))
    parser.add_argument("--keywords", type=int, default=100, help="æœ€å¤§å…³é”®è¯æ•°é‡ (é»˜è®¤100)")
    parser.add_argument("--quiet", action="store_true", help="ç®€åŒ–è¾“å‡ºæ¨¡å¼")
    parser.add_argument("--list-sources", action="store_true", help="æ˜¾ç¤ºæ”¯æŒçš„æ–°é—»æº")
    
    args = parser.parse_args()
    
    # æ˜¾ç¤ºæ”¯æŒçš„æ–°é—»æº
    if args.list_sources:
        logger.info("æ”¯æŒçš„æ–°é—»æºå¹³å°:")
        for source, name in SOURCE_NAMES.items():
            logger.info(f"  {source:<25} {name}")
        return
    
    # éªŒè¯å‚æ•°
    if args.keywords < 1 or args.keywords > 200:
        logger.error("å…³é”®è¯æ•°é‡åº”åœ¨1-200ä¹‹é—´")
        sys.exit(1)
    
    # è¿è¡Œæå–
    try:
        success = asyncio.run(run_extraction_command(
            sources=args.sources,
            keywords_count=args.keywords,
            show_details=not args.quiet
        ))
        
        sys.exit(0 if success else 1)
        
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(1)

if __name__ == "__main__":
    main()
